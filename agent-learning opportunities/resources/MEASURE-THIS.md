# Measure This: A Lightweight Playbook for Making Your Team's Learning Experiment More Visible & Valued in Your Org 

I've heard from a number of folks that they're trying the Learning Opportunities skill with their teams. Yay! 

Running a learning experiment is exactly the kind of collaborative culture-building that research shows matters for software teams. If you're interested in layering on some evidence strategy to the learning experiment, I wrote this playbook to help you run a simple pre/post check-in with your team, interpret what you find, and package it into something that makes your team look good. You can use this as a starting place to think about capturing insight from your team experiments, holding yourselves accountable to the practice, and setting some shared team goals that are about learning outcomes, not just velocity. 

## Why Measure?

People can determine whether using the Learning Opportunities Skill is useful to them on their own. You don't need to prove statistically significant differences emerge over a week of using the Skill with your team of six (and you probably shouldn't even use that as the metric, more on that below). 

However, it can be useful to experiment with this because:

- It gives your team a structured way to reflect on how the experiment felt
- It turns a week of trying something new into a visible, shareable story
- It connects your team's experience to a broader body of peer-reviewed research on developer thriving which might spark new ideas for things to try 
- It can make the intangible gains of your culture work (learning, confidence, psychological safety around AI) more legible and defensible to your leadership who weren't in the room. 

## The Measures

These items come from the open-access measures we developed from an empirical social science study of 3,267 professional software developers, and from a study with 1,282 professional software developers, representing 12+ industries. They are validated across an adult (18+) population and were developed in English (although they achieved good benchmarks with participants answering from a large range of countries, see full paper for details), open access, and free to use under a CC-BY-SA 4.0 license. Both studies also included a wide range of demographics and industry and experience contexts. 

**Full citations:** 
Hicks, C. M., Lee, C. S., & Foster-Marks, K. (2024). The New Developer: AI Skill Threat, Identity Change & Developer Thriving in the Transition to AI-Assisted Software Development. https://doi.org/10.31234/osf.io/2gej5

Hicks, C. M., Lee, C. S., & Ramsey, M. (2024). Developer Thriving: four sociocognitive factors that create resilient productivity on software teams. IEEE Software, 41(4), 68-77. doi: 10.1109/MS.2024.3382957

You can find the complete set of measures and more detailed design notes in [the AI Skill Threat open access measures supplement](https://osf.io/preprints/psyarxiv/2gej5) and in the [the Developer Thriving online supplement](https://ieeexplore.ieee.org/ielx7/52/10547603/10491133/supp1-3382957.pdf?arnumber=10491133).

I've selected a subset of 8 items that are most relevant to what you're likely to see shift during a week-long learning experiment, and which I've found most useful in consulting with real working teams. This takes about 2 minutes to complete. You can use all of them, pick the ones that resonate, or add your own â€” but keep it short enough that people will actually do it twice.

### Core Items

**Learning Culture** (DTS-LC; included in both the Developer Thriving Scale and the AI Skill Threat study)

*Scale: 1 = Strongly Disagree, 2 = Somewhat Disagree, 3 = Neither Agree nor Disagree, 4 = Somewhat Agree, 5 = Strongly Agree*

1. Overall at work, I feel like I am learning new skills and growing as a software engineer.
2. On my team, we often share new things we have learned with each other.

**AI Skill Threat** (PAST; Perceived AI Skill Threat in Software Engineering)

*Scale: 1 = Strongly Disagree, 2 = Somewhat Disagree, 3 = Neither Agree nor Disagree, 4 = Somewhat Agree, 5 = Strongly Agree*

3. When I think about how generative AI will change software development, I feel anxious or uneasy.
4. Because of generative AI tools or AI capabilities, I worry that many of the skills I currently use as a software engineer will become obsolete very quickly.

**Coding Self-Efficacy** (CSE)

*Scale: 1 = Strongly Disagree to 5 = Strongly Agree*

5. Even when I have frustrating or unexpected problems while working with code, I know I will be able to solve them.

**AI Behavioral Action** (AI-BA)

*Scale: 1 = Not at all, 2 = Somewhat, 3 = Moderately, 4 = Very, 5 = Extremely*

6. How likely are you to seek out ways to practice and apply new skills for using AI in software development in the near future?

### Optional/More Complex Add-Ons

NOTE, because the following items tap more into larger psychological affordances, the following items are harder to move, and less likely to change on a lightweight one week experiment. I recommend running these items over at least a month, and over several repeated timepoints. However, if you want a fuller picture and have an anonymized way to administer these questions (because they can feel a bit more sensitive), I'd consider also including:

**Sense of Belonging** (M-SBS)

*Scale: 1 = Strongly Disagree to 5 = Strongly Agree*

7. Currently, I am supported to grow, celebrate, learn, and make mistakes by my team.
8. Currently, I am accepted for who I am by my team.

**Developer Agency** (DTS-A) Note, this comes from [the DTS as published in this paper](https://ieeexplore.ieee.org/abstract/document/10491133) 

9. If I disagree with how we are defining success as a team, I feel I am able to voice that disagreement honestly.
10. As a developer, I feel that I have a voice in the ways my code work and technical contributions are measured.

**Team Effectiveness** (TER)

*Scale: 1 = Not at all effective, 2 = Slightly effective, 3 = Moderately effective, 4 = Very effective, 5 = Extremely effective*

11. Overall, I would rate my current software team as: ___

### Attribution / Authorship

These measures are provided under a **CC-BY-SA 4.0** license. You are welcome to use, remix, and share them with attribution. If you adapt them, please credit the original authors and link to the study. My ability to keep sharing open scientific work depends on thoughtful attribution â€” we would appreciate it if you link to our project and use our names in any adaptation or sharing. For the Learning Opportunities Skill, credit Cat Hicks. For the empirical research study findings, credit [Cat Hicks](https://www.drcathicks.com/), [Carol Lee](https://www.carol-s-lee-phd.com/) and Kristen Foster-Marks.

## How to Run It

**Timing:**
- **Pre:** Before your team starts using the Learning Opportunities skill; beginning of the week is typical. 
- **Post:** After the experiment ends; I suggest keeping it lightweight while still long enough for people to calibrate over variable tasks, a week is great. 

**Format options I've seen work well:**
- **Google Form or similar** â€” anonymous, structured, easiest to compare. Copy the items above into a form with the response scales as radio buttons.
- **Team channel poll** â€” lower barrier, less private. Fine if your team culture supports open sharing, but will.
- **Individual reflection** â€” each person jots down their own ratings. You can share or keep private.

**Tips:**
- Keep it anonymous if you can. People answer differently when they know their manager will see individual responses. However, emphasize that individual items are *naturally* going to shift and change over time.
- Use the exact same items and scales both times. Don't reword things between pre and post.
- Don't discuss the "expected" results before people take the post survey. You want their genuine experience, not social desirability.

## What to Do with Your Results

### What you CAN do

**Look at averages as well as the distributions of scores.** For each item, average the team's pre scores and the team's post scores. You now have a direction of movement for each construct. However, average scores will be more or less meaningful depending on the team context. 

**Look at direction, not magnitude.** Did learning culture go up? Did AI skill threat go down? Did self-efficacy hold steady or increase? The *direction* is informative even when the *amount* isn't statistically testable.

**Use it as a discussion prompt.** The most valuable part of this exercise can be simply discussing it among the team. Don't be afraid to let people criticize the measures themselves. They're not fixed in stone, they're just ways you tried to reflect on your experience. Try: "Our ratings of coding self-efficacy really seemed higher. Does that match how it felt? What do you think drove that change?" or "Learning culture didn't move much even though we thought it would. Was there something that made it hard to engage with learning this week?"

**Compare your team's averages to the study benchmarks.** L\Our original study reports means and distributions for all of these measures across 3,267 developers. You can talk about where your team sits relative to the broader population. That context can be powerful regardless of the direction of difference. Treat this as a talking point, not a referendum (there may be perfectly valid reasons you look different from our research sample, not least of which because the AI tools themselves are changing constantly): "Our team's learning culture is above the average" or "We're seeing higher AI skill threat than the study sample, I wonder why."

It can be very helpful to look at spread/overall patterns, not just the center. If your team's average AI Skill Threat went from 3.5 to 3.2, that's interesting â€” but it matters a lot whether everyone moved from ~3.5 to ~3.2, or whether one person dropped from 5 to 1 while everyone else stayed the same. An easy way to think about this is to always report the range or standard deviation alongside the average. In software development research, within-person and between-person variance is often enormous relative to the effects of any single factor (you can read our large-scale research *and* use the code from it at the reference Flournoy, Lee, Wu & Hicks, 2025). A single observation is a very noisy signal of what's typical. This applies to your team's survey responses too.

### What you probably should NOT do

**Do not fixate on p-values or even run statistical significance tests on your team of 5-8 people.** A t-test on n=6 is not meaningful when you do not have the statistical power to detect real effects, and any p-value you get will be uninterpretable â€” a non-significant result does NOT mean nothing happened, and a significant result would be unreliable. This is not a limitation of your team or your experiment. However, if you have a larger team or a justified situation for statistical testing, you can certainly go for it. 

**Do not treat individual scores as a comprehensive diagnostic.** These measures describe patterns across groups. One person's score of 2 on AI Skill Threat doesn't mean they aren't threatened; one person's 5 on Learning Culture doesn't mean they're thriving. Aggregates are more stable than individual data points. I would treat a lightweight pre- and post- measurement as a team experiment about the *environment.* 

**Do not treat individual measures as fixed, immutable traits**

We built these measures to help teams reflect on what they feel is supported and most emphasized by their teams. If you read our whole paper, you'll see that these measures are grounded in empirical psychology that is about *beliefs and cultures that can change*, not about identifying fixed and immutable traits of individuals.

**Do not conclude "the intervention didn't work" from flat or mixed results.** A one-week experiment with a small team is a pilot. There are many reasons you might not see movement: the timeframe was too short, the team was already high on learning culture, people were distracted by other work, the measures don't capture the specific kind of change that happened. Absence of measurable change is not evidence of absence of impact. Qualitative impact can be just as valuable, and point you toward changes that will eventually show up in the measures.

## Common Mistakes When Using AI to Analyze This

I'm currently building a companion analysis skill to help with responsible interpretation of this kind of data. It's not ready yet, but here's what you need to know right now:

**If you ask an AI assistant to "analyze my results," it will almost certainly do something wrong.** Not because the tool is bad, but because its default behavior is to be helpful, and being helpful with small-sample data often means telling you a confident story that isn't warranted by the evidence. It can also default to common statistical misconceptions, so it's important to think about creating verifiable analysis code which you can then test, rather than simply ask "analyze our data."

Here's what tends to go wrong, as I've gathered from not only my experimenting with Claude but also chatting with scientist friends about common mistakes we see:

ðŸš© **Spurious significance testing.** Asking for statistical testing over small numbers can be misleading, especially when you are underpowered to detect effects and sensitive to outliers. However, common norms are to emphasize p-values and statistical significance. Sticking with descriptive findings is almost always more appropriate for small teams. Below, I suggest some additional nudges you might use/or learn about (effect sizes, practical significance). 

ðŸš© **Confabulated norms/giving you the interpretation rather than you making the interpretation.** When you ask "what does a score of 3.2 on AI Skill Threat mean?" Claude can confidently tells you it indicates "moderate anxiety" or is "within the normal range," but be making that up to plausibly fit what scientific conversations can sound like. It likely doesn't have access to the actual distribution from our study, and even if it did, norm interpretation typically requires understanding of the measurement context that goes beyond looking up a number. 

ðŸš© **Overly-confident causal language from descriptive data, too-precise estimates, and overconfidence in interventions.** Claude will tend to push confident causal claims and estimates, like "the Learning Opportunities skill reduced AI Skill Threat by 18%." But if you ran a pre/post with no control group, you cannot necessarily always attribute the change to the skill, which is why we build our understanding from many experiments rather than one single test. Maybe your team had a good week, or a stressful project ended, and that also opened up the chance for them to try the Learning Opportunity Skill. Maybe people got more comfortable with each other. While pre/post designs can describe change they don't isolate causes. I'm not going to stand in your way making a reasonable inference about what helped in your real life, but I would keep in mind that it's *hard to change human psychology and many good interventions still fail in the real world,* [more on this in this paper](https://pmc.ncbi.nlm.nih.gov/articles/PMC8928154/pdf/nihms-1772354.pdf). I would expect any single point estimate to be variable, so you should never treat an estimate from a one-time survey as a magic benchmark.  

**What to do instead:**

If you're not comfortable with statistics, there's nothing wrong with sticking with descriptive summaries like averages, direction of change, range of responses, or the number of people on your team who had a positive change at all vs the number who had a negative change. Use the numbers to start conversations, not to end them. If you want to go deeper or need expert support on evidence design, that's the kind of thing I help teams with â€” [reach out](https://www.catharsisinsight.com/). If you are comfortable with statistics or just want to dive deeper into the world of applied research and your own learning, [I recommend this free resource](https://experimentology.io/). 

## Write Code, Not Just Prompts

If you do want to use Claude or another AI tool to help you work with your data, consider writing a reusable analysis script instead of just pasting your data into a chat and asking for conclusions. Because just as with the Learning Opportunities Skill itself, **the act of building the analysis teaches you something that "just get me the answer" doesn't.**

A simple Python or R script that calculates pre/post averages for each measure, computes the difference, and prints a clean summary is:

- **Reusable** â€” run it again next month when you repeat the experiment
- **Testable** â€” you can see exactly what calculations are being done
- **Shareable** â€” other teams can use your script for their own experiments
- **A learning opportunity** â€” writing the analysis is itself a chance to understand what the numbers mean

You can absolutely ask Claude to help you write this script. I just recommend that you stay in the driver's seat. 

## Some Claude nudges you might consider adding to your Claude.md or elsewhere if you are using it to design for more complex analysis

### Statistical Rigor
- When exploring data, present findings plainly without hyperbole 
- Flag potential confounds, alternative explanations, and threats to validity proactively
- Distinguish between statistical significance and practical significance 
- Think about effect size, interaction effects, and multilevel models
- Always characterize variance and spread alongside central tendency â€” standard deviations, ranges, and distributions matter as much as means
- Disaggregate within-person and between-person effects when working with repeated measures; these can differ in sign and magnitude (Curran & Bauer, 2011; Flournoy, Lee, Wu & Hicks, 2025)
- When an effect is "statistically precise but practically small," say so. Small effects set against large unexplained variance means any single observation provides limited signal and suggest we need to incorporate other missing measures
- Consider whether the outcome variable's distribution is appropriate for the analysis (e.g., when data is not normally distributed assuming it is can bias results)

### Research Mindset
- Consider construct validity seriously â€” what are we actually measuring vs. what we claim to measure?
- Think about selection effects, reverse causality, and omitted variable bias
- Recognize that high variance is itself a finding, not just noise to explain away. It tells us about the complexity of the system being measured
- Be skeptical of "silver bullet" interpretations: if a single factor appears to explain a large share of a complex outcome, interrogate that before celebrating it

### Communication
- Don't simplify statistical concepts 
- Be direct and substantive; skip reassurance in favor of accuracy
- Evidence-based information over optimism
- When I ask about a finding, I want to understand mechanisms, not just that an effect exists
- When reporting results from small samples, lead with uncertainty and descriptive context rather than point estimates

## The Team Boast

If your experiment went well â€” or even if it was just interesting, I **highly recommend** you package it up. In my research with software teams, I have regularly found that less than one in four developers agree their organizations truly see and value their technical work. For more guidance, [I dove deeper into this in a project about why developers feel like they can't learn out loud](https://www.catharsisinsight.com/science).

Think about using a short paragraph in an update email, a Slack message to your skip-level, a slide in your team's next review. Here's a template I've given friends:

---

> This week, our team ran a structured learning experiment to invest in our skill development while working with AI coding tools. We used the Learning Opportunities skill [link to repo] to build deliberate practice into our AI-assisted workflow, and tracked our experience using validated measures from large-scale research on developer thriving that found learning culture was significantly associated with developers' ability to ramp up to AI (Hicks, Lee & Foster-Marks, 2024; https://osf.io/preprints/psyarxiv/2gej5_v2).
>
> Over the course of the week, we saw [describe what you observed â€” e.g., "an increase in our team's learning culture scores," "a decrease in anxiety about AI skill changes," "strong engagement with the exercises, with team members sharing what they learned"]. We also had a team discussion about [what came up â€” e.g., "what kinds of AI-generated code we understand well vs. what we're just accepting without scrutiny"] and got an unexpected insight [elevate a suggestion from your team].
>
> We plan to [continue / expand / repeat] this experiment in the coming weeks.

---

Of course, you need to adapt this to your context and voice, but the key things I'd highlight are that you did something real, it's grounded in external evidence, and you can articulate why it matters for your organization's AI journey. 

You can find me (and more scientific work) at [drcathicks.com](https://drcathicks.com) or [@grimalkina on Bluesky](https://bsky.app/profile/grimalkina.bsky.social).

---

These measures and this playbook are shared under a CC-BY-SA 4.0 license. 

Please attribute measures and AI Skill Threat model to: Hicks, C. M., Lee, C. S., & Foster-Marks, K. (2025, March 15). The New Developer: AI Skill Threat, Identity Change & Developer Thriving in the Transition to AI-Assisted Software Development. https://doi.org/10.31234/osf.io/2gej5_v2

## References 

Curran, P. J., & Bauer, D. J. (2011). The disaggregation of within-person and between-person effects in longitudinal models of change. Annual review of psychology, 62(1), 583-619. https://doi.org/10.1146/annurev.psych.093008.100356 

Flournoy, J.C., Lee, C.S., Wu, M., Hicks, C. No silver bullets: Why understanding software cycle time is messy, not magic. Empirical Software Engineering 30, 174 (2025). https://doi.org/10.1007/s10664-025-10735-w

Frank, M. C., Braginsky, M., Cachia, J., Coles, N. A., Hardwicke, T. E., Hawkins, R. D., Mathur, M. B., & Williams, R. 2025. Experimentology: An Open Science Approach to Experimental Psychology Methods. Stanford University. https://doi.org/10.25936/3JP6-5M50. (Also published by MIT Press, ISBN 978-0-262-55256-1)

Hicks, C. M., Lee, C. S., & Foster-Marks, K. (2025, March 15). The New Developer: AI Skill Threat, Identity Change & Developer Thriving in the Transition to AI-Assisted Software Development. https://doi.org/10.31234/osf.io/2gej5_v2

Hicks, C. M., Lee, C. S., & Ramsey, M. (2024). Developer Thriving: four sociocognitive factors that create resilient productivity on software teams. IEEE Software, 41(4), 68-77. doi: 10.1109/MS.2024.3382957

